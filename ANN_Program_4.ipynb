{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f29bf7",
   "metadata": {},
   "source": [
    "Program 4: Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34551a",
   "metadata": {},
   "source": [
    "A basic neural network mimics the structure and function of the human brain to perform tasks like classification, regression, and pattern recognition. It consists of layers of interconnected nodes (neurons), where each connection has an associated weight. Here's a step-by-step explanation of how a basic neural network works:\n",
    "\n",
    "Components of a Neural Network\n",
    "\n",
    "Neurons (Nodes):\n",
    "\n",
    "The fundamental units of the network.\n",
    "Each neuron receives input, processes it, and passes the output to the next layer.\n",
    "Layers:\n",
    "\n",
    "Input Layer: The first layer that receives input data.\n",
    "\n",
    "Hidden Layer(s): Intermediate layers that process inputs received from the input layer.\n",
    "\n",
    "Output Layer: The final layer that produces the network's output.\n",
    "Weights:\n",
    "\n",
    "Each connection between neurons has a weight that determines the strength and direction of the signal passed.\n",
    "Biases:\n",
    "\n",
    "Each neuron has an associated bias value added to the weighted sum of inputs before applying the activation function.\n",
    "\n",
    "Activation Function:\n",
    "\n",
    "A non-linear function applied to the weighted sum of inputs plus the bias.\n",
    "Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n",
    "\n",
    "How a Basic Neural Network Works\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Weights and biases are initialized, often randomly.\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "Input Layer: Takes input data and passes it to the first hidden layer.\n",
    "\n",
    "Hidden Layers: Each neuron in a hidden layer computes a weighted sum of its inputs, adds the bias, and applies an activation function. The output is passed to the next layer.\n",
    "\n",
    "Output Layer: Produces the final output of the network by processing the inputs from the last hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93250f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        np.random.seed(42)\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        \n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self.sigmoid(self.final_input)\n",
    "        \n",
    "        return self.hidden_output, self.final_output\n",
    "\n",
    "    def backpropagation(self, inputs, hidden_output, final_output, expected_output, learning_rate):\n",
    "        output_error = expected_output - final_output\n",
    "        output_delta = output_error * self.sigmoid_derivative(final_output)\n",
    "        \n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(hidden_output)\n",
    "        \n",
    "        self.weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
    "        \n",
    "        self.weights_input_hidden += inputs.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate\n",
    "\n",
    "    def train(self, inputs, expected_output, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            hidden_output, final_output = self.forward_propagation(inputs)\n",
    "            self.backpropagation(inputs, hidden_output, final_output, expected_output, learning_rate)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        _, final_output = self.forward_propagation(inputs)\n",
    "        return final_output\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        pred_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(labels, axis=1)\n",
    "        return np.mean(pred_labels == true_labels)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the network\n",
    "nn = NeuralNetwork(input_size=4, hidden_size=5, output_size=3)\n",
    "nn.train(X_train, y_train, learning_rate=0.1, epochs=10000)\n",
    "\n",
    "# Test the network\n",
    "test_results = nn.predict(X_test)\n",
    "print(\"Test Accuracy:\", nn.accuracy(test_results, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f86caf",
   "metadata": {},
   "source": [
    "BACKPROPAGATION ALGORITHM:\n",
    "\n",
    "backpropagation function updates the weights and biases of the neural network based on the error between the predicted output (final_output) and the expected output (expected_output).\n",
    "\n",
    "output_error is the difference between the expected output and the actual output.\n",
    "\n",
    "output_delta is the gradient of the loss function with respect to the output, scaled by the derivative of the activation function (sigmoid in this case). This delta represents how much the output layer's weights need to be adjusted.\n",
    "\n",
    "hidden_error is the propagated error from the output layer back to the hidden layer. It is calculated by taking the dot product of output_delta and the transpose of the weights connecting the hidden and output layers.\n",
    "\n",
    "hidden_delta is the gradient of the loss function with respect to the hidden layer's outputs, scaled by the derivative of the activation function (sigmoid in this case). This delta represents how much the hidden layer's weights need to be adjusted.\n",
    "\n",
    "self.weights_hidden_output is updated by adding the product of the transposed hidden layer output and output_delta, scaled by the learning rate. This adjusts the weights in the direction that reduces the output error.\n",
    "\n",
    "self.bias_output is updated by adding the sum of output_delta across all samples, scaled by the learning rate. This adjusts the biases in the output layer.\n",
    "\n",
    "\n",
    "self.weights_input_hidden is updated by adding the product of the transposed input data and hidden_delta, scaled by the learning rate. This adjusts the weights in the direction that reduces the hidden layer error.\n",
    "\n",
    "self.bias_hidden is updated by adding the sum of hidden_delta across all samples, scaled by the learning rate. This adjusts the biases in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f4ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
